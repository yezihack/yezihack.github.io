<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>云运维笔记 on</title><link>https://yezihack.github.io/categories/%E4%BA%91%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/</link><description>Recent content in 云运维笔记 on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 05 Jul 2023 11:41:00 +0800</lastBuildDate><atom:link href="https://yezihack.github.io/categories/%E4%BA%91%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>Haproxy + Keepalived 实现 k8s 集群高可用</title><link>https://yezihack.github.io/posts/haproxy-keepalived/</link><pubDate>Wed, 05 Jul 2023 11:41:00 +0800</pubDate><guid>https://yezihack.github.io/posts/haproxy-keepalived/</guid><description>1. 什么是 Kubernetes 的高可用 高可用性是指系统或应用程序在面对故障或异常情况时能够保持持续运行和提供服务的能力。在构建高可用的Kubernetes集群时，可以采取以下一些高级方法和策略：
多节点部署：使用多个节点来部署Kubernetes集群，确保即使某个节点发生故障，其他节点仍然可以继续提供服务。
负载均衡：通过在集群前端引入负载均衡器，将流量分发到多个节点上，实现负载均衡和故障转移。这样即使某个节点发生故障，负载均衡器可以将流量重新路由到其他健康的节点上。
自动伸缩：利用Kubernetes的自动伸缩功能，根据实际负载情况自动调整集群的节点数量，以满足应用程序的需求。这样可以在高负载时增加节点数量，保证性能，而在低负载时减少节点数量，节省资源。
容器健康检查：通过定义容器的健康检查机制，Kubernetes可以监控容器的运行状态，并在容器出现故障或异常时自动重启或替换容器，确保应用程序的持续可用性。
数据备份和恢复：定期对关键数据进行备份，并建立可靠的数据恢复机制，以防止数据丢失或损坏。这可以通过使用Kubernetes的持久化存储卷（Persistent Volume）和备份工具来实现。
故障切换和容错：通过使用Kubernetes的故障切换功能，可以在节点或容器发生故障时自动将服务切换到备用节点或容器上，确保应用程序的连续性和可用性。
监控和告警：建立全面的监控系统，实时监测集群和应用程序的运行状态，并设置告警机制，及时发现和处理潜在的故障或异常情况。
通过采取这些高级方法和策略，可以有效地提高Kubernetes集群的可用性和稳定性，确保应用程序在运行时不会出现服务中断。
2. Haproxy + Keepalived 优缺点 Haproxy和Keepalived是常用的组合，用于实现负载均衡和高可用性的解决方案。下面是它们的优缺点：
2.1. Haproxy Haproxy的优点：
高性能：Haproxy是一个高性能的负载均衡器，能够处理大量的并发连接和请求。 灵活的配置：Haproxy提供了丰富的配置选项，可以根据需求进行灵活的负载均衡策略和规则配置。 健康检查：Haproxy支持对后端服务器进行健康检查，可以自动排除故障的服务器，确保只将请求转发到健康的服务器上。 SSL终止：Haproxy可以作为SSL终止器，将SSL/TLS连接解密后再转发给后端服务器，减轻服务器的负担。 Haproxy 的缺点：
单点故障：Haproxy本身是单点，如果Haproxy节点发生故障，可能会导致服务中断。 配置复杂：Haproxy的配置相对复杂，需要一定的学习和经验来正确配置和管理。 2.2. Keepalived Keepalived 的优点：
高可用性：Keepalived可以将多个Haproxy节点组成一个高可用集群，通过VRRP协议实现故障切换，确保服务的连续性和可用性。 快速故障切换：Keepalived能够快速检测到主节点的故障，并将VIP（虚拟IP）迅速切换到备用节点上，减少服务中断时间。 简单配置：Keepalived的配置相对简单，可以快速部署和管理。 Keepalived的缺点：
配置同步：Keepalived需要确保配置文件的同步，以保证所有节点的配置一致性，这可能需要额外的配置和管理工作。 依赖性：Keepalived依赖于底层网络和操作系统的支持，可能受限于特定的网络环境和操作系统版本。 架构图 3. Haproxy 3.1. 底层原理 Keepalived 是一种用于实现高可用性的软件，其底层原理主要包括以下几个方面：
VRRP 协议：Keepalived 使用 VRRP（Virtual Router Redundancy Protocol）协议来实现高可用性。VRRP 协议通过将多个服务器组成一个虚拟路由器组，共同提供相同的虚拟 IP 地址，实现了服务器的冗余备份和故障切换。
虚拟路由器组：多个服务器通过 Keepalived 组成一个虚拟路由器组，共同提供相同的虚拟 IP 地址。其中一个服务器被选举为主服务器（Master），其他服务器为备份服务器（Backup）。
心跳检测：主备服务器之间通过周期性的心跳消息进行通信，以检测主服务器的可用性。如果备份服务器在一定时间内没有收到主服务器的心跳消息，就会触发主备切换。
健康检查：Keepalived 支持对服务的健康检查，通过定期检查服务的可用性和性能，及时剔除故障或不可用的服务器，保证了服务的高可用性和稳定性。
路由表更新：当主服务器发生故障或不可用时，备份服务器中的一台将会被选举为新的主服务器，接管虚拟 IP 地址的转发功能。同时，Keepalived 会更新路由表，将虚拟 IP 地址指向新的主服务器。</description></item><item><title>Istio 限流实现</title><link>https://yezihack.github.io/posts/istio-ratelimit/</link><pubDate>Mon, 12 Jun 2023 15:29:48 +0800</pubDate><guid>https://yezihack.github.io/posts/istio-ratelimit/</guid><description>1. 介绍限流 限流是一种通过对系统请求进行限制和控制，避免系统过载，保证系统稳定性和安全性的技术手段。
2. Istio 限流 首先确认需要限流的应用是否已经加载了 sidecar，如果还未安装 istio, 请参考:https://yezihack.github.io/istio-install.html 创建 app-ratelimit.yaml：
设置流速间隔时间：token_bucket.fill_interval 设置流速令牌数量：token_bucket.max_tokens 选择哪些应用限流：workloadSelector.labels apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: app-ratelimit spec: workloadSelector: labels: app: my-app # 用来选择需要进行配置的工作负载 configPatches: - applyTo: HTTP_FILTER match: listener: filterChain: filter: name: &amp;#34;envoy.filters.network.http_connection_manager&amp;#34; patch: operation: INSERT_BEFORE value: name: envoy.filters.http.local_ratelimit typed_config: &amp;#34;@type&amp;#34;: type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit value: stat_prefix: http_local_rate_limiter token_bucket: # 令牌桶算法的配置信息，用于控制每秒放行的请求数量。 max_tokens: 10 # 指定令牌桶中最多可以存储的令牌数，即最大可用令牌数 tokens_per_fill: 10 # 指定每次填充令牌桶的令牌数，即每次可用令牌数。 fill_interval: 60s # 定填充令牌桶的时间间隔，即每隔多长时间填充一次令牌桶。 filter_enabled: # 控制是否启用该过滤器的开关。 runtime_key: local_rate_limit_enabled default_value: numerator: 100 denominator: HUNDRED filter_enforced: # 制是否强制执行该过滤器的开关。 runtime_key: local_rate_limit_enforced default_value: numerator: 100 denominator: HUNDRED response_headers_to_add: - append: false header: key: x-local-rate-limit value: &amp;#39;true&amp;#39; kubectl apply -f app-ratelimit.</description></item><item><title>Istio 安装</title><link>https://yezihack.github.io/posts/istio-install/</link><pubDate>Mon, 12 Jun 2023 14:26:54 +0800</pubDate><guid>https://yezihack.github.io/posts/istio-install/</guid><description>1. Istio 介绍 Istio 是由 Google、IBM 和 Lyft 开源的微服务管理、保护和监控框架。Istio 为希腊语，意思是”起航“。
Istio 使用功能强大的 Envoy 服务代理扩展了 Kubernetes，以建立一个可编程的、可感知的应用程序网络。
Istio 与 Kubernetes 和传统工作负载一起使用，为复杂的部署带来了标准的通用流量管理、遥测和安全性。
2. 下载 istio 以 CentOS7 为例
官方下载：https://github.com/istio/istio/releases/
截止写本文时，版本更新到 v1.18.0。
2.1. 版本的选择 https://istio.io/latest/zh/docs/releases/supported-releases/
v1.18.0 适合 k8s 1.24, 1.25, 1.26, 1.27。
根据你的 kubernetes 版本进行下载相应的 istio 版本。
cd /opt/src wget https://github.com/istio/istio/releases/download/1.18.0/istio-1.18.0-linux-amd64.tar.gz tar -zxvf istio-1.18.0-linux-amd64.tar.gz cd istio-1.18.0 cp istio-1.18.0/bin/istioctl /usr/local/bin # 查看版本 istioctl version # 如果显示如下，则表示你未设置 KUBECONFIG 环境变量 unable to retrieve Pods: Get &amp;#34;http://localhost:8080/api/v1/namespaces/istio-system/pods?fieldSelector=status.phase%3DRunning&amp;amp;labelSelector=app%3Distiod&amp;#34;: dial tcp [::1]:8080: connect: connection refused 1.</description></item><item><title>云运维笔记(10) Etcd V3.2 集群二进制搭建</title><link>https://yezihack.github.io/posts/etcd-v3.2/</link><pubDate>Tue, 16 May 2023 11:24:46 +0800</pubDate><guid>https://yezihack.github.io/posts/etcd-v3.2/</guid><description>1. 准备工作 搭建 ETCD 高可用集群，至少3台或5台或7台，奇数台即可。本地搭建采用3台 Linux CentOS7.9 环境。
序列 HOSTNAME IP etcd 节点名称 1 kube-10 192.168.9.10 etcd01 2 kube-11 192.168.9.11 etcd02 3 kube-13 192.168.9.13 etcd03 创建目录：
bin 存储 etcd 二进制文件 data 存储数据目录 sh 脚本目录 ssl 证书目录 # 每个机器上都执行 mkdir -p /opt/etcd/{bin,data,sh,ssl} 2. 证书生成 2.1. cfssl 工具 cd /opt/src # 下载 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 # 改名 mv cfssl_linux-amd64 cfssl mv cfssljson_linux-amd64 cfssljson mv cfssl-certinfo_linux-amd64 cfssl-certinfo # 添加执行权限 chmod +x cfssl cfssljson cfssl-certinfo # 复制到 /usr/local/bin cp cfssl cfssl-cerinfo cfssljson /usr/local/bin 2.</description></item><item><title>云运维笔记(11) Etcd V3.4 集群二进制搭建</title><link>https://yezihack.github.io/posts/etcd-v3.4/</link><pubDate>Tue, 16 May 2023 11:24:46 +0800</pubDate><guid>https://yezihack.github.io/posts/etcd-v3.4/</guid><description>1. 准备工作 适合于 kubernetes 1.17/1.18/1.19/1.20/1.21 搭建 ETCD 高可用集群，至少3台或5台或7台，奇数台即可。本地搭建采用3台 Linux CentOS7.9 环境。
序列 HOSTNAME IP etcd 节点名称 1 kube-10 192.168.9.10 etcd01 2 kube-11 192.168.9.11 etcd02 3 kube-13 192.168.9.13 etcd03 创建目录：
bin 存储 etcd 二进制文件 data 存储数据目录 conf 配置目录 sh 脚本目录 ssl 证书目录 # 每个机器上都执行 mkdir -p /opt/etcd-3.4/{bin,data,sh,ssl,conf} 时间同步 # 安装 yum install chrony -y # 管理 systemctl start chronyd #启动 systemctl status chronyd #查看 systemctl restart chronyd #重启 systemctl stop chronyd #停止 systemctl enable chronyd #设置开机启动 # 修改时区 timedatectl set-timezone Asia/Shanghai # 设置完时区后，强制同步下系统时钟： chronyc -a makestep 设置与指定服务器时间同步</description></item><item><title>K8s 跨 Node 机器 Pod 网络异常</title><link>https://yezihack.github.io/posts/k8s-bug-pod-network/</link><pubDate>Tue, 07 Feb 2023 18:07:36 +0800</pubDate><guid>https://yezihack.github.io/posts/k8s-bug-pod-network/</guid><description>背景 在 k8s 里部署了应用需要通过 ingress 提供外部调用. ingressController Pod 应用部署在 A 机器上, 应用部署在 B 机器上.
通过自定义域名调用应用,则需要经过自定义域名配置的 host 的 kube-proxy 到 IngressController Pod 机器,再由 Ingress 负载找到应用的 Service 负载的 endpoint.
最终请求到应用的 Pod.
环境 k8s: 1.19 docker: 19.10 linux: CentOS7.6 分析思路 先确定 host:port 端口是否通达？ 再确认 ingress 是否可以访问到 service IP ingress 关于作者 我的博客：https://yezihack.github.io
欢迎关注我的微信公众号【空树之空】，一日不学则面目可憎也，吾学也。</description></item><item><title>云运维笔记(9) Kubernetes Pod 调度策略</title><link>https://yezihack.github.io/posts/k8s-dispatch/</link><pubDate>Fri, 16 Dec 2022 18:14:20 +0800</pubDate><guid>https://yezihack.github.io/posts/k8s-dispatch/</guid><description>1.1. 四大调度方式 1.1.1. 自动调试 1.1.2. 定向调度 1.1.2.1. NodeName 1.1.2.2. NodeSelector 1.1.3. 亲和性调度 1.1.3.1. NodeAffinity 1.1.3.2. PodAffinity 1.1.3.3. PodAntiAffinity 1.1.4. 污点（容忍）调度 1.1.4.1. 污点 1.1.4.2. 容忍 1.2. 参考 关于作者 1.1. 四大调度方式 自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出 定向调度：NodeName、NodeSelector 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity 污点（容忍）调度：Taints、Toleration 1.1.1. 自动调试 完全交由 kube-scheduler 来决定 pod 调度到哪里，不受人为控制。
1.1.2. 定向调度 1.1.2.1. NodeName NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。
使用实例：
apiVersion: v1 kind: Pod metadata: name: pod-nodename namespace: dev spec: nodeName: kube-11 # 指定调度到node1节点上 containers: - name: nginx image: nginx:1.17.1 tip: 这种调度不够灵活，必须指定某 node 节点，若 node 异常会导致调度失败。</description></item><item><title>云运维笔记(8) Kubeadm 内网补丁版本升级，从v1.16.0至v1.16.15</title><link>https://yezihack.github.io/posts/kubeadm-upgrade-v1.16/</link><pubDate>Fri, 09 Dec 2022 16:26:37 +0800</pubDate><guid>https://yezihack.github.io/posts/kubeadm-upgrade-v1.16/</guid><description>.1. 为什么升级 漏洞问题 使用新功能 .2. 特殊性 内网环境，没有外网。 多 master 集群。 外置 Etcd。 .3. 版本 kubeadm升级前版本：v1.16.0 kubeadm升级后版本：v1.16.15 .4. 升级前的检查 .4.1. 查看当前版本 kubeadm version kubeadm version: &amp;amp;version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;16&amp;#34;, GitVersion:&amp;#34;v1.16.0&amp;#34;, GitCommit:&amp;#34;72c30166b2105cd7d3350f2c28a219e6abcd79eb&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-01-18T23:29:13Z&amp;#34;, GoVersion:&amp;#34;go1.13.5&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} .4.2. 离线下载 kubectl,kubeadm,kubelet 下载 # 必须本机没有安装以下软件 version=&amp;#34;1.16.15&amp;#34; yumdownloader --resolve --destdir=/opt/local-packages/ kubelet-${version} kubeadm-${version} kubectl-${version} .4.3. 制作共享 YUM 源 使用工具：saber
假定本机IP：192.168.10.10
# 安装 Createrepo yum install createrepo -y createrepo /opt/local-packages/ # 如果存在 repodata 则使用更新 createrepo --update /opt/local-packages/ # 共享文件 saber fs /opt/local-packages/ .</description></item><item><title>云运维笔记(7) kubernetes 错误收集</title><link>https://yezihack.github.io/posts/k8s-error/</link><pubDate>Wed, 07 Dec 2022 11:31:00 +0800</pubDate><guid>https://yezihack.github.io/posts/k8s-error/</guid><description>.1. 为什么 Kubernetes 众多组件汇集于一身，插件也是多如牛毛，在运维中或日常安装中难免会遇到各种各样的错误，有些错误并不好排查，让人火急火燎搜索一翻，半天已经过去。在此收集日常使用 kubernetes 遇到的问题。
.2. Ingress-nginx .2.1. Internal error occurred: failed calling webhook &amp;ldquo;validate.nginx.ingress.kubernetes.io&amp;rdquo; .2.1.1. 详细错误信息 Error from server (InternalError): error when creating &amp;#34;ingress.yaml&amp;#34;: Internal error occurred: failed calling webhook &amp;#34;validate.nginx.ingress.kubernetes.io&amp;#34;: Post &amp;#34;https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1beta1/ingresses?timeout=10s&amp;#34;: context deadline exceeded .2.1.2. 解决方法 删除验证
# 查看 kubectl get validatingwebhookconfigurations # 删除 kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission .3. 关于作者 我的博客：https://yezihack.github.io
欢迎关注我的微信公众号【空树之空】，一日不学则面目可憎也，吾学也。</description></item><item><title>云运维笔记(6) k1s 工具使用教程</title><link>https://yezihack.github.io/posts/k1s/</link><pubDate>Tue, 06 Dec 2022 16:46:57 +0800</pubDate><guid>https://yezihack.github.io/posts/k1s/</guid><description>.1. k1s 是 kubectl 辅助工具 .2. 什么是 k1s .3. k1s 特色 .4. 安装 .5. 快速上手 .6. 功能 .6.1. Resources 列表( kubectl 系统对应) .6.2. Resources 列表(扩展功能) .6.3. Action 列表 .6.4. Extend 扩展功能 .6.5. 环境变量 .7. 使用说明 .7.1. 设置环境变量 .7.2. 日志查看 .7.3. 进入容器 .7.4. 资源操作 .7.4.1. nodes 资源 .7.4.2. pods 资源 .7.4.3. deployments 资源 .7.4.4. daemonsets 资源 .7.4.5. services 资源 .7.4.6. 清理垃圾 .8. 关于作者 .1. k1s 是 kubectl 辅助工具 so easy, so fast.</description></item><item><title>云运维笔记(5) Kubernetes GPU 支持与驱动安装</title><link>https://yezihack.github.io/posts/k8s-gpu/</link><pubDate>Wed, 23 Nov 2022 08:42:25 +0800</pubDate><guid>https://yezihack.github.io/posts/k8s-gpu/</guid><description>.1. 安装 GPU 驱动 .1.1. 查看 GPU 硬件 .1.2. 检查自带 GPU 驱动 .1.3. 官方下载 GPU 驱动 .1.4. 安装 GPU 驱动 .1.5. 安装失败 .1.5.1. ERROR: An NVIDIA kernel module &amp;rsquo;nvidia-uvm&amp;rsquo; appears to already be loaded in your kernel .1.5.2. ERROR: Unable to find the kernel source tree for the currently running kernel .1.6. 重装内核 .1.7. 再次安装 GPU 驱动包 .1.7.1. 安装 .run 文件 .1.7.2. 安装 .rpm 文件 .2. 安装 nvidia-container-runtime .2.1. 作用 .</description></item><item><title>云运维笔记(4) Kubeadm etcd 堆叠式安装 k8s 1.20</title><link>https://yezihack.github.io/posts/kubeadm-install-v1.20/</link><pubDate>Mon, 14 Nov 2022 18:27:51 +0800</pubDate><guid>https://yezihack.github.io/posts/kubeadm-install-v1.20/</guid><description>.1. 回顾 .2. 安装前的准备 .2.1. 安装要求 .2.2. 集群规划 .2.3. 版本选择 .2.4. 基本设置 .3. Kubernetes 设置的参数 .3.1. br_netfilter 模块 .3.2. 桥接的IPv4流量传递到iptables的链 .3.3. 加载 IPVS .4. Docker 部署 .4.1. 设置 Docker 镜像源 .4.2. 列出 Docker 所有的版本 .4.3. 安装 docker .4.4. 设置 daemon.json .4.4.1. 设置CPU .4.4.2. 支持GPU .4.5. 启动 docker .5. 设置 firewall 防火墙规则 .5.1. k8s master需要开启以下端口 .5.2. k8s node需要开启以下端口 .5.3. 打开 NAT 转发功能 .5.4. calico 需要开启以下端口 .5.5. NFS 防火墙规则设置 .5.6. 其它端口 .</description></item><item><title>云运维笔记(3) k8s 安装 dashboard 配置 ingress</title><link>https://yezihack.github.io/posts/k8s-dashboard-ingress/</link><pubDate>Tue, 08 Nov 2022 16:47:30 +0800</pubDate><guid>https://yezihack.github.io/posts/k8s-dashboard-ingress/</guid><description>.1. 回顾 .2. 下载对应的版本 .3. 生成自签名的证书 .4. 生成 secret .5. 修改 dashboard.yaml 文件 .6. 部署 Dashboard .7. 创建 token .7.1. 创建 admin token .7.2. 创建某空间的 token .8. 配置 ingress-nginx .9. dashboard 登陆使用 .10. 参考 .11. 关于作者 .1. 回顾 之前写过一篇 kuberntes-dashboard 的文章，介绍如何使用 nodeport 方式部署与访问。
参考：第十一章 Kubernetes Dashboard
本次介绍使用 ingress 域名方式访问 dashboard。
采用 tls 方式配置 ingress-nginx 访问 dashboard。 .2. 下载对应的版本 访问 github 仓库：https://github.com/kubernetes/dashboard/
如何安装合适自己 kuberntes 版本的 Dashboard 的呢？官方发布 release 时，每个版本都有测试，当前版本支持哪些范围的 kubernetes 版本。还特意列出不完全兼容的版本信息。</description></item><item><title>云运维笔记(2) Kubeadm etcd 堆叠式安装 k8s 1.16</title><link>https://yezihack.github.io/posts/kubeadm-install-v1.16/</link><pubDate>Thu, 04 Aug 2022 10:40:28 +0800</pubDate><guid>https://yezihack.github.io/posts/kubeadm-install-v1.16/</guid><description>.1. Kubeadm 高可用集群 本次安装 Kubernetes 采用官方推荐的 kubeadm 安装方式。
利用 kubeadm 创建高可用集群，使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：
使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。 使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。 本次教程采用 etcd 堆叠式高可用集群，即将 etcd 与控制平面的节点在同一个位置。
.2. 安装前的准备 .2.1. 安装要求 在开始安装 kubernetes 集群机器之前需要满足以下几上条件：
序列 名称 参考值 命令 1 系统 Linux uname -s 2 内存 &amp;gt;= 2 GB free -hm 3 CPU &amp;gt;= 2 核 cat /proc/cpuinfo |grep &amp;ldquo;processor&amp;rdquo;|wc -l 4 硬盘 &amp;gt;= 20 GB df -h 5 交换分区 必须禁用 swapoff / vim /etc/fstab 6 网络 集群中所有机器之间网络互通 ping 7 主机名 集群中所有机器不重复 hostname 8 MAC地址 集群中所有机器不重复 cat /sys/class/net/ens33/address 9 product_uuid 集群中所有机器不重复 cat /sys/class/dmi/id/product_uuid .</description></item><item><title>云运维笔记(1) CentOS7 安装</title><link>https://yezihack.github.io/posts/centos-install/</link><pubDate>Wed, 03 Aug 2022 16:54:31 +0800</pubDate><guid>https://yezihack.github.io/posts/centos-install/</guid><description>.1. 准备 CentOS7 镜像 .2. Vmware Workstation 安装 CentOS7 .3. 配置虚拟机网络 .4. 配置 CentOS 网络 .5. 更改 CentOS7 Yum源 .6. 安装常用的软件 .7. 安装 Oh-my-zsh .8. 克隆系统 .9. 关于作者 .1. 准备 CentOS7 镜像 使用清华大学开源镜像站下载 CentOS7: https://mirrors.tuna.tsinghua.edu.cn/centos/7/isos/x86_64/
CentOS-7-x86_64-DVD-2009.iso 标准安装版 CentOS-7-x86_64-Everything-2009.iso 完整版，集成所有软件 CentOS-7-x86_64-Minimal-2009.iso 精简版，自带的软件最少 CentOS-7-x86_64-NetInstall-2009.iso 网络安装版（从网络安装或者救援系统) 本次安装教程使用精简版镜像。因为无须图形界面，也无须自带太多的软件。
.2. Vmware Workstation 安装 CentOS7 操作相对比较简单，下一步即可。
新建虚拟机 选择镜像源 下一步 调整配置 内存调整为 2GB。 CPU调整为 2 个处理器，每个处理器内核数量为 2。 开启此虚拟机 .3. 配置虚拟机网络 网络规划：192.168.9.0/24 虚拟机采用 NAT 模式连接宿主机网络 菜单 -&amp;gt; 编辑 -&amp;gt; 虚拟网络编辑器：</description></item></channel></rss>